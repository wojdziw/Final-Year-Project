\contentsline {section}{\numberline {1}Introduction}{1}
\contentsline {section}{\numberline {2}Background - machine intelligence}{3}
\contentsline {subsection}{\numberline {2.1}Neural networks}{3}
\contentsline {subsection}{\numberline {2.2}Gradient descent}{7}
\contentsline {subsection}{\numberline {2.3}Deep learning}{8}
\contentsline {section}{\numberline {3}Scaling and parallelisation}{9}
\contentsline {section}{\numberline {4}Existing methods}{11}
\contentsline {subsection}{\numberline {4.1}Large Scale Distributed Deep Networks}{11}
\contentsline {subsection}{\numberline {4.2}Project Adam}{13}
\contentsline {subsection}{\numberline {4.3}Contrast and conclusions}{14}
\contentsline {section}{\numberline {5}ADMM}{14}
\contentsline {subsection}{\numberline {5.1}ADMM and deep learning}{16}
\contentsline {subsection}{\numberline {5.2}Strategy}{16}
\contentsline {subsection}{\numberline {5.3}Dual net}{16}
\contentsline {section}{\numberline {6}Architecture}{18}
\contentsline {subsection}{\numberline {6.1}Putting it all together}{18}
\contentsline {subsection}{\numberline {6.2}Dataset}{20}
\contentsline {subsection}{\numberline {6.3}Deep learning frameworks}{20}
\contentsline {subsection}{\numberline {6.4}Caffe tricks and quirks}{21}
\contentsline {section}{\numberline {7}The algorithm}{24}
\contentsline {section}{\numberline {8}Experimental setup}{25}
\contentsline {subsection}{\numberline {8.1}Technical difficulties}{26}
\contentsline {subsection}{\numberline {8.2}Two objectives}{26}
\contentsline {subsection}{\numberline {8.3}Net 1 loss}{27}
\contentsline {subsection}{\numberline {8.4}Default Euclidean Loss Layer in practice}{28}
\contentsline {subsection}{\numberline {8.5}Custom Euclidean Loss Layer}{30}
\contentsline {subsubsection}{\numberline {8.5.1}No normalization}{30}
\contentsline {subsubsection}{\numberline {8.5.2}Corrected custom setup}{32}
\contentsline {subsubsection}{\numberline {8.5.3}Normalization term }{33}
\contentsline {subsection}{\numberline {8.6}Rethinking ADMM}{34}
\contentsline {subsubsection}{\numberline {8.6.1}Back to the default, non-normalized approach}{34}
\contentsline {subsubsection}{\numberline {8.6.2}Individual convergence}{35}
\contentsline {subsection}{\numberline {8.7}Adjusting the parameters}{36}
\contentsline {subsubsection}{\numberline {8.7.1}Batch size}{37}
\contentsline {subsubsection}{\numberline {8.7.2}Learning rate}{38}
\contentsline {subsection}{\numberline {8.8}Final setup}{39}
\contentsline {subsubsection}{\numberline {8.8.1}Batch size}{39}
\contentsline {subsubsection}{\numberline {8.8.2}Learning rate}{40}
\contentsline {subsubsection}{\numberline {8.8.3}Number of training epochs}{40}
\contentsline {subsubsection}{\numberline {8.8.4}Other parameters}{41}
\contentsline {subsubsection}{\numberline {8.8.5}Dataset and Caffe input setup}{41}
\contentsline {subsubsection}{\numberline {8.8.6}Parameter summary}{42}
\contentsline {section}{\numberline {9}Results}{42}
\contentsline {subsection}{\numberline {9.1}Evaluation metrics}{42}
\contentsline {subsection}{\numberline {9.2}Classification accuracy}{43}
\contentsline {subsection}{\numberline {9.3}Loss function}{44}
\contentsline {subsection}{\numberline {9.4}Training time}{45}
\contentsline {subsection}{\numberline {9.5}Maximum batch size}{47}
\contentsline {subsection}{\numberline {9.6}Memory efficiency}{47}
\contentsline {subsection}{\numberline {9.7}Another dataset}{48}
\contentsline {section}{\numberline {10}Conclusions}{48}
\contentsline {subsection}{\numberline {10.1}Future recommendations}{50}
