\contentsline {section}{\numberline {1}Introduction}{1}
\contentsline {section}{\numberline {2}Background - machine intelligence}{3}
\contentsline {subsection}{\numberline {2.1}Neural networks}{3}
\contentsline {subsection}{\numberline {2.2}Gradient descent}{6}
\contentsline {subsection}{\numberline {2.3}Deep learning}{8}
\contentsline {section}{\numberline {3}Scaling and parallelisation}{9}
\contentsline {section}{\numberline {4}Breaking it down}{10}
\contentsline {subsection}{\numberline {4.1}Large Scale Distributed Deep Networks}{11}
\contentsline {subsection}{\numberline {4.2}Project Adam}{12}
\contentsline {subsection}{\numberline {4.3}Contrast and conclusions}{13}
\contentsline {section}{\numberline {5}ADMM}{14}
\contentsline {section}{\numberline {6}ADMM and deep learning}{15}
\contentsline {subsection}{\numberline {6.1}Strategy}{16}
\contentsline {subsection}{\numberline {6.2}Dual net}{16}
\contentsline {section}{\numberline {7}Architecture}{18}
\contentsline {subsection}{\numberline {7.1}Putting it all together}{18}
\contentsline {subsection}{\numberline {7.2}Dataset}{20}
\contentsline {subsection}{\numberline {7.3}Deep learning frameworks}{20}
\contentsline {subsection}{\numberline {7.4}Caffe tricks and quirks}{21}
\contentsline {section}{\numberline {8}The algorithm}{23}
\contentsline {section}{\numberline {9}Experimental setup}{25}
\contentsline {subsection}{\numberline {9.1}Technical difficulties}{25}
\contentsline {subsection}{\numberline {9.2}Two objectives}{26}
\contentsline {subsection}{\numberline {9.3}Net 1 loss}{27}
\contentsline {subsection}{\numberline {9.4}Default Euclidean Loss Layer in practice}{28}
\contentsline {subsection}{\numberline {9.5}Custom Euclidean Loss Layer}{30}
\contentsline {subsubsection}{\numberline {9.5.1}No normalization}{30}
\contentsline {subsubsection}{\numberline {9.5.2}Corrected custom setup}{32}
\contentsline {subsubsection}{\numberline {9.5.3}Normalization term }{32}
\contentsline {subsection}{\numberline {9.6}Rethinking ADMM}{34}
\contentsline {subsubsection}{\numberline {9.6.1}Back to the default, non-normalized approach}{34}
\contentsline {subsubsection}{\numberline {9.6.2}Individual convergence}{35}
\contentsline {subsection}{\numberline {9.7}Adjusting the parameters}{35}
\contentsline {subsubsection}{\numberline {9.7.1}Batch size}{36}
\contentsline {subsubsection}{\numberline {9.7.2}Learning rate}{37}
\contentsline {subsection}{\numberline {9.8}Final setup}{39}
\contentsline {subsubsection}{\numberline {9.8.1}Batch size}{39}
\contentsline {subsubsection}{\numberline {9.8.2}Learning rate}{39}
\contentsline {subsubsection}{\numberline {9.8.3}Number of training epochs}{40}
\contentsline {subsubsection}{\numberline {9.8.4}Other parameters}{40}
\contentsline {subsubsection}{\numberline {9.8.5}Dataset and Caffe input setup}{41}
\contentsline {subsubsection}{\numberline {9.8.6}Parameter summary}{41}
\contentsline {section}{\numberline {10}Results}{42}
\contentsline {subsection}{\numberline {10.1}Evaluation metrics}{42}
\contentsline {subsection}{\numberline {10.2}Classification accuracy}{43}
\contentsline {subsection}{\numberline {10.3}Loss function}{44}
\contentsline {subsection}{\numberline {10.4}Training time}{44}
\contentsline {subsection}{\numberline {10.5}Maximum batch size}{46}
\contentsline {subsection}{\numberline {10.6}Another dataset}{47}
\contentsline {section}{\numberline {11}Conclusions}{47}
\contentsline {subsection}{\numberline {11.1}Future recommendations}{49}
