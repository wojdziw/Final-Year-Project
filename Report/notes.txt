GRADIENT DESCENT DESCRIPTION

The loss minimization problems often involve gradient methods, such as gradient descent. To see why that is, we note that wherever we find outselves on the graph of the function we can find the local minimum neighbouring our current location by following the negative gradient, i.e. the direction where the function is monotonously decreasing. Once we hit a point where we can't minimize any further, we are at the minimum. Gradient descent does precisely that - it first calculates the derivative of the function and follows the direction that will result in the loss minimization, this can be clearly seen in the figure \ref{fig:gradientdescent} below.

NICE IMAGE OF GRADIENT DESCENT

NICE EQUATION

If we calculate the gradient of the loss function based on all of the points, we use a variant of the gradient descent algorithm called "batch" gd. It turns out, though, that such a method is extremely computationally engaging, particularly noting how large the datasets can be nowadays!

Instead, we can use a technique called the "stochastic" gradient descent. Its correctness is guaranteed by the assumption that if we use a random sample of the whole training set many times, the computed gradients will average out to give a globally correct answer. In an extreme case, we only use one data point to calculate the gradient, however for stability purposes we can use image batches of up to a few hundred images.

NICE EQUATION





Recent years have seen a tremendous rate of development of the field of deep learning, and, not surprisingly, a surge in the number of deep learning frameworks. Today the choice is abundant, and the main competitors vary significantly when it comes to performance, implementation and usability. Due to the very technical, low-level nature of the project, significant consideration was given to the choice of an appropriate framework.

The most significant frameworks nowadays include caffe, theano, torch and tensorflow by google. There is also a number of overlying interfaces and wrappers, such as Keras. The prototype was decided to be built in caffe for several reasons:

\begin{itemize}
\item It has got a well-developed python interface and api
\item It seems to be endorsed as not having a very steep learning curve
\item There is a big community support for it
\end{itemize}



All of the above certainly come at a cost, though, which in the caffe manifests itself through:

\begin{itemize}
\item As much as it is well supported, caffe is not very well documented. It's hard to dig into the intricacies of its python API at times.
\item Even the python interface requires the usage of prototxt files to define the model architecture and solver details. That reduces the clarity of the overall codebase.
\end{itemize}



With all of its pros and cons weighted up, caffe was decided to be the right tool for prototyping, and hence it was used throughout the first stages kf the project, It is absolutely possible and rather advisable to check the feasibility of the desrcibed solutions in a different framework.





Caffe tricks and quirks

Even with the architecture meticulously planned out and the objectives very clear, the implementation of the prototype turned out to be a very challenging tasks. Sadly (or not...), this was not due to the inconsistencies in the model laid out in the previous sections, but rather due to the erratic documentation of caffe. The most challenging issues encountered during prototyping were:

\begin{enumerate}
\item \textbf{Running the forward and backward pass of net2 separately}. The default caffe interface for carrying out one iteration of learning is "solver.net.step(1)" and it runs a forward and backward pass through the net as well as the weight update. As it turns out, this is surprisingly very difficult to do in steps using the predefined functions.
\item \textbf{Updating the input of net2}. As mentioned earlier, the input to net2 is dynamic - it changes with every iteration, because the output of the pool2 layer changes. It is, again, surprisingly difficult to achieve the desired effect.
\end{enumerate}



Fortunately, extensive research of the caffe websites and fora helped solve many of the above problems, notably (in the same order):

\begin{enumerate}
\item Caffe interface offers the "solver.net.forward()" and "solver.net.backward() functions which complete two of the three required iteration steps, with the other one being the weight update. Even though caffe offers little automation for that, it can and was done manually, by investigating the "blob.diff" value for each of the parameter matrices (blobs in this case). The "diff" contains the gradient of a specific weight with respect to the overall loss function, and hence if multiplied by the negative learning rate it should return the needed weight update:
\begin{equation}
w^{t+1} = w^t + \Delta w = w^t - \mu \fraction{\delta L}{\delta w}
\end{equation}
\item At first, the dynamic data layer update was done manually by just overwriting the contents of a randomly initialized layer.  It turned out to be a flawed approach because caffe does not allow for a manual update of an operational data layer. Even though the values seem to be updated when queried, calling the forward pass function brings them back to the original values. Caffe does have several types of data layers, though, and one of them - NAME OF THE LAYER - turned out to be particularly useful for our application. The data layer is not initialized until the desired data is manully fed into it in the python code, which also allows for dynamic updates between iterations. This also preserves the updated data when forward and backward passes are called.
\end{enumerate}



A very similar technique to the one described above was used for communicating the layer conv3p (or conv3') data to net1. The communication was required because the loss of net1 is defined as the norm difference between the dual conv3 and conv3p layers. The data was hence simply copied from net2 and pasted manually into a LAYERNAME data layer in net1 before the loss computation.



There are several ways that could be used to implement the \textbf{network communication}. Initial drafts of the framework revolved around using tcp sockets to send the appropriate data in an in-order, error-free manner. Fortunately, the architecture of the server cluster used for the training took advantage of a shared file system, which could be accessed from each of the distinct processing units. That is why the desired parameters were just saved as numpy ".npy" files and easily accessed by the other part of the algorithm at the appropriate stage.

One more technical nuance we should mention is the synchronization between the networks. Similarly to the method above, the nets saved the details about their current computation stages in a shared file that could be probed if needed.



\subsection{The algorithm}

After introducing all the necessary theoretical, architectural and practical insights about the requirements of the project we are now able to design the algorithm used for the dual net training. As stated before, the algorithm is going to try to imitate the \textbf{alternating direction method of multipliers} algorithm. As the name suggests, we are then going to try to optimize the dual nets separately in an alternating manner - that means we will focus on minimizing the loss function of only one of them at any one time. This is achieved using the following algorithm:

HERE TAKE THE PDF OF THE CODE SLIDE AND PUT IT HERE

The first step of the algorithm is the forward pass of network 1. This produces the initial layer activations and, importantly, the output of the pool2 layer. This is saved into a file and communicated to the rest of the program running on a separate GPU.

For the second step we move to net2. It first retrieves the data communicated by net1 earlier and loads in onto its data layer. It then runs a complete training iteration including forward and backward pass followed by the parameters update. The backward pass minimizes the difference between the image labels computed during the forward pass and the ground truth. Net2 then saves the data computed for the dual conv3' layer.

We now move back to net1. It first loads the conv3' layer data and computes the norm difference between the dual conv3 and conv3' layers. This is net1's cost function, and hence during the backward propagation the solver calculates parameter gradients that lead to minimization of the norm. After the backprop is done, we are ready to carry out the final pass through the network and use the computed gradients to update the weights appropriately.

After the weight update one complete iteration of the whole setup is done. We have hopefully:

\begin{enumerate}
\item Minimized the difference between the dual layers.
\item Minimized the overall loss, which is akin to training the single network (and should make the two indistinguishable from the outside world).
\end{enumerate}

We can then repeat the above procedure for a few thousand iterations or until some convergence criterion is met.

\subsection{Testing}

The meticulously planned setup above then required a thorough heuristic validation. The hardware used to test the algorithms was a single Nvidia LOLOL Graphical Processing Unit for each network, hence two of them in total, placed on two distinct servers within a cluster.

Naturally, many problems were encounted before the network started exhibiting the desired behaviour. Most of them were linked to the issues described in an earlier section, namely:

\begin{itemize}
\item Net2 was not converging because the dynamic data layer update was not behaving as expected.
\item Net1 was not converging because the parameters were not appropriately updated based on the computed gradients.
\end{itemize}

The solutions to these are mentioned in detail above as well.

Once initial performance metrics started looking positive, i.e. both the net1 and net2 losses decreased as the training proceeded, some baseline reults and conclusions could have been drawn.

The losses computed after each of the iterations seem to be very volatile. Before we jump into the exact analysis of the problem, let's revisit the idea of the stochastic gradient descent. The gradient of the loss function computed by SGD is based on a random sample of the training set, which in our case is 256 images out of 10,000. This is clearly not representative of the whole population on its own, however over many iterations it should converge to a representative average. This is precisely what happens in our case. The losses computed after each iteration seem volatile, however over many iterations they do exhibit convergence. The stochastic batches we use for updating the loss are big enough for global minimization over a long time, but too small to produce a consistent trend at every iteration. This should not be a concern, though, as long as the general trend is visible.

The point above mainly concerns both net1 and net2, however it is clearly more pronounced in the latter, hence more consideration is given into the possible explanations. Due to the nature of the algorithm, we are building up gradually more volatile calculations, hence the loss of net2 can be expected to vary more. In particular, it should be noted that the input of net2 is not constant, but varying with every iteration adding to the overall volatility.

After all, both of the nets seem to be converging, which can be clearly seen in figure \ref{fig:losses}. It is clear, however, that the convergence of net2 is rather slow and volatile, hence more care has to be taken into fine-tuning its parameters in order to to achieve a more smooth, significant trend.

