\relax 
\citation{dean2012large}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{kruger2013deep}
\citation{ng2015coursera}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background - machine intelligence}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural networks}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces {A logistic unit}\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:logistic_unit}{{2.1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces {A simple neural net}\relax }}{4}}
\newlabel{fig:neural_net}{{2.2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gradient descent}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces {We follow the function in the negative gradient direction.}\relax }}{7}}
\newlabel{fig:graddesc}{{2.3}{7}}
\newlabel{weight_update}{{2.19}{7}}
\citation{kruger2013deep}
\citation{stanford2016convnets}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Deep learning}{8}}
\citation{ciresan2010deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces {The input volume in red is transformed into a set of 5 activation layers using 5 convolutional filters.}\relax }}{9}}
\newlabel{fig:conv_net}{{2.4}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Scaling and parallelisation}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Breaking it down}{10}}
\newlabel{breakingitdown}{{4}{10}}
\citation{dean2012large}
\citation{chilimbi2014project}
\citation{dean2012large}
\citation{dean2012large}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Large Scale Distributed Deep Networks}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Example model architecture in DistBelief \cite  {dean2012large}\relax }}{11}}
\newlabel{fig:distbelief}{{4.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Project Adam}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Contrast and conclusions}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {5}ADMM}{14}}
\newlabel{ADMM}{{5}{14}}
\newlabel{ADMM_equation}{{5.5}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {6}ADMM and deep learning}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Strategy}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Dual net}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces {The single network architecture used as a baseline.}\relax }}{16}}
\newlabel{fig:net12}{{6.1}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces {Breaking down the network between the machines}\relax }}{18}}
\newlabel{fig:dual-training}{{6.2}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Architecture}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Putting it all together}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces {The single network architecture used as a baseline.}\relax }}{18}}
\newlabel{fig:net12pretty}{{7.1}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces {The dual networks imitating the single one.}\relax }}{19}}
\newlabel{fig:net12separate}{{7.2}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces {The Caffe net1 print.}\relax }}{19}}
\newlabel{fig:net1}{{7.3}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces {The Caffe net2 print.}\relax }}{20}}
\newlabel{fig:net1}{{7.4}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Dataset}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Deep learning frameworks}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Caffe tricks and quirks}{21}}
\newlabel{sec:caffe_tricks}{{7.4}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {8}The algorithm}{23}}
\newlabel{sec:algorithm}{{8}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces {The algorithm used for dual network training.}\relax }}{24}}
\newlabel{fig:algorithm}{{8.1}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Experimental setup}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Technical difficulties}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Two objectives}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Net 1 loss}{27}}
\newlabel{euclidean_loss_formula}{{9.2}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Default Euclidean Loss Layer in practice}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces {The training losses for the prototype framework.}\relax }}{29}}
\newlabel{fig:net2_no_convergence_default}{{9.1}{29}}
\citation{caffe2016loss}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Custom Euclidean Loss Layer}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1}No normalization}{30}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}{Defining the loss in a custom loss layer.}}{30}}
\newlabel{lst:custom_loss}{{1}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces {The losses for both of the networks during the first iteration of training using a custom Euclidean loss layer.}\relax }}{31}}
\newlabel{fig:incorrect_custom_training}{{9.2}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.2}Corrected custom setup}{32}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.3}Normalization term }{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces {The training losses with the "default" custom layer in place.}\relax }}{33}}
\newlabel{fig:net2_no_convergence_custom}{{9.3}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6}Rethinking ADMM}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.1}Back to the default, non-normalized approach}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.2}Individual convergence}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7}Adjusting the parameters}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces {The modified version of the training algorithm. Networks 1 and 2 converge separately, with one training and the other idling at one chosen time.}\relax }}{36}}
\newlabel{fig:algorithm_revisited}{{9.4}{36}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.7.1}Batch size}{36}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.7.2}Learning rate}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces {An illustration of the learning rate variation. The function on the left exhibits slow, yet optimal convergence. In the middle the convergence is much faster, but virtually non-existent because the algorithm misses the optimal point. The best of both worlds is presented in the rightmost figure. The learning rate is adaptive, and slows down when minimum is approached.}\relax }}{38}}
\newlabel{fig:learning_rate_overshoot}{{9.5}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.8}Final setup}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.8.1}Batch size}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.8.2}Learning rate}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.8.3}Number of training epochs}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.8.4}Other parameters}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.8.5}Dataset and Caffe input setup}{41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.8.6}Parameter summary}{41}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The parameters used for baseline result generation.\relax }}{41}}
\newlabel{tab:parameters}{{1}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Results}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Evaluation metrics}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Classification accuracy}{43}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The classification accuracies\relax }}{43}}
\newlabel{tab:accuracies}{{2}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Loss function}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Training time}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces {The characteristic "step" pattern exhibited by the neural network. During one sequence, each of the nets optimizes its loss function and then hands the training over to the net that was idling before.}\relax }}{45}}
\newlabel{fig:step_pattern}{{10.1}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces {The final loss convergence pattern for the single and dual network architectures. The step pattern for the training of the dual network is clear and visible, although the convergence trend is very pronounced and dominating over the idling periods.}\relax }}{45}}
\newlabel{fig:final_trace}{{10.2}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Maximum batch size}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces {The final loss convergence pattern for the single and dual network architectures. The step pattern is now removed and only the non-idling iterations' losses are shown.}\relax }}{46}}
\newlabel{fig:final_trace_nosteps}{{10.3}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6}Another dataset}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Conclusions}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Future recommendations}{49}}
\bibdata{bib}
\bibcite{chilimbi2014project}{1}
\bibcite{ciresan2010deep}{2}
\bibcite{dean2012large}{3}
\bibcite{kruger2013deep}{4}
\bibcite{stanford2016convnets}{5}
\bibcite{ng2015coursera}{6}
\bibcite{caffe2016loss}{7}
\bibstyle{abbrv}
