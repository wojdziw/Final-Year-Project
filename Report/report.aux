\relax 
\citation{kruger2013deep}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{ng2015coursera}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background - machine intelligence}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural networks}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces {A logistic unit}\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:logistic_unit}{{2.1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces {A simple neural net}\relax }}{4}}
\newlabel{fig:neural_net}{{2.2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gradient descent}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces {We follow the function in the negative gradient direction.}\relax }}{6}}
\newlabel{fig:graddesc}{{2.3}{6}}
\citation{kruger2013deep}
\citation{stanford2016convnets}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Deep learning}{7}}
\citation{ciresan2010deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces {The input volume in red is transformed into a set of 5 activation layers using 5 convolutional filters.}\relax }}{8}}
\newlabel{fig:conv_net}{{2.4}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Scaling and parallelisation}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Breaking it down}{9}}
\newlabel{breakingitdown}{{4}{9}}
\citation{dean2012large}
\citation{chilimbi2014project}
\citation{dean2012large}
\citation{dean2012large}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Large Scale Distributed Deep Networks}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Example model architecture in DistBelief \cite  {dean2012large}\relax }}{10}}
\newlabel{fig:distbelief}{{4.1}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Project Adam}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Contrast and conclusions}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}ADMM}{12}}
\newlabel{ADMM}{{5}{12}}
\newlabel{ADMM_equation}{{5.5}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {6}ADMM and deep learning}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Strategy}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Dual net}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces {The single network architecture used as a baseline.}\relax }}{14}}
\newlabel{fig:net12}{{6.1}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces {Breaking down the network between the machines}\relax }}{16}}
\newlabel{fig:dual-training}{{6.2}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Architecture}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Putting it all together}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces {The single network architecture used as a baseline.}\relax }}{16}}
\newlabel{fig:net12pretty}{{7.1}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces {The dual networks imitating the single one.}\relax }}{17}}
\newlabel{fig:net12separate}{{7.2}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces {The Caffe net1 print.}\relax }}{17}}
\newlabel{fig:net1}{{7.3}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces {The Caffe net2 print.}\relax }}{18}}
\newlabel{fig:net1}{{7.4}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Dataset}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Deep learning frameworks}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Caffe tricks and quirks}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {8}The algorithm}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces {The algorithm used for dual network training.}\relax }}{21}}
\newlabel{fig:algorithm}{{8.1}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Testing}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Technical difficulties}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Two objectives}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Net 1 loss}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Default Euclidean Loss Layer in practice}{24}}
\citation{caffe2016loss}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces {The training losses for the prototype framework.}\relax }}{26}}
\newlabel{fig:default_losses}{{10.1}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Custom Euclidean Loss Layer}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces {The losses for both of the networks during the first iteration of training using a custom Euclidean loss layer.}\relax }}{27}}
\newlabel{fig:incorrect_custom_training}{{10.2}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Big batch}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces {Training curves of the single and dual network setup using maximum batch sizes (2608 samples in single and 4096 samples in dual).}\relax }}{29}}
\newlabel{fig:big_batch}{{10.3}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6}Normalization term }{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces {It turned out that introducing the normalization term helped bound the loss values a little, but still led to very unstable behaviour afterwards}\relax }}{31}}
\newlabel{fig:exploding_values}{{10.4}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.7}Learning rate}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces {In the pursuit of the errors, several different network setups were tested. This one involved changing the sign of the gradient addition.}\relax }}{32}}
\newlabel{fig:plus_equals}{{10.5}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces {It is clear that in most of the cases varying the learning rate changed just that - the rate at which we are approaching convergence. The above nets were trained using the same parameters and under identical circumstances, save the learning rate which differed by a factor of a 100 between the two.}\relax }}{33}}
\newlabel{fig:learning_rate_variation}{{10.6}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.8}things to do in this report}{34}}
\bibdata{bib}
\bibcite{chilimbi2014project}{1}
\bibcite{ciresan2010deep}{2}
\bibcite{dean2012large}{3}
\bibcite{kruger2013deep}{4}
\bibcite{stanford2016convnets}{5}
\bibcite{ng2015coursera}{6}
\bibcite{caffe2016loss}{7}
\bibstyle{abbrv}
