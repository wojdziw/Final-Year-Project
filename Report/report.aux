\relax 
\citation{dean2012large}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{kruger2013deep}
\citation{ng2015coursera}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background - machine intelligence}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural networks}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces {A logistic unit}\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:logistic_unit}{{2.1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces {A simple neural net}\relax }}{4}}
\newlabel{fig:neural_net}{{2.2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gradient descent}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces {We follow the function in the negative gradient direction.}\relax }}{7}}
\newlabel{fig:graddesc}{{2.3}{7}}
\citation{kruger2013deep}
\citation{stanford2016convnets}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Deep learning}{8}}
\citation{ciresan2010deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces {The input volume in red is transformed into a set of 5 activation layers using 5 convolutional filters.}\relax }}{9}}
\newlabel{fig:conv_net}{{2.4}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Scaling and parallelisation}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Breaking it down}{10}}
\newlabel{breakingitdown}{{4}{10}}
\citation{dean2012large}
\citation{chilimbi2014project}
\citation{dean2012large}
\citation{dean2012large}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Large Scale Distributed Deep Networks}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Example model architecture in DistBelief \cite  {dean2012large}\relax }}{11}}
\newlabel{fig:distbelief}{{4.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Project Adam}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Contrast and conclusions}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {5}ADMM}{14}}
\newlabel{ADMM}{{5}{14}}
\newlabel{ADMM_equation}{{5.5}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {6}ADMM and deep learning}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Strategy}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Dual net}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces {The single network architecture used as a baseline.}\relax }}{16}}
\newlabel{fig:net12}{{6.1}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces {Breaking down the network between the machines}\relax }}{18}}
\newlabel{fig:dual-training}{{6.2}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Architecture}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Putting it all together}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces {The single network architecture used as a baseline.}\relax }}{18}}
\newlabel{fig:net12pretty}{{7.1}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces {The dual networks imitating the single one.}\relax }}{19}}
\newlabel{fig:net12separate}{{7.2}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces {The Caffe net1 print.}\relax }}{19}}
\newlabel{fig:net1}{{7.3}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces {The Caffe net2 print.}\relax }}{20}}
\newlabel{fig:net1}{{7.4}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Dataset}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Deep learning frameworks}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Caffe tricks and quirks}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {8}The algorithm}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces {The algorithm used for dual network training.}\relax }}{24}}
\newlabel{fig:algorithm}{{8.1}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Testing}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Technical difficulties}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Two objectives}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Net 1 loss}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Default Euclidean Loss Layer in practice}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces {The training losses for the prototype framework.}\relax }}{29}}
\newlabel{fig:first_try}{{9.1}{29}}
\citation{caffe2016loss}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Custom Euclidean Loss Layer}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1}No normalization}{30}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}{Defining the loss in a custom loss layer.}}{30}}
\newlabel{lst:custom_loss}{{1}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces {The losses for both of the networks during the first iteration of training using a custom Euclidean loss layer.}\relax }}{31}}
\newlabel{fig:incorrect_custom_training}{{9.2}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.2}Corrected custom setup}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces {The training losses with the "default" custom layer in place.}\relax }}{32}}
\newlabel{fig:second_try}{{9.3}{32}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.3}Normalization term }{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6}Rethinking ADMM}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.1}Back to the default, non-normalized approach}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.2}Individual convergence}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7}Adjusting the parameters}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.7.1}Batch size}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces {Training curves of the single and dual network setup using maximum batch sizes (2608 samples in single and 4096 samples in dual). CHANGE ME}\relax }}{35}}
\newlabel{fig:big_batch}{{9.4}{35}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.7.2}Learning rate}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.8}things to do in this report}{37}}
\bibdata{bib}
\bibcite{chilimbi2014project}{1}
\bibcite{ciresan2010deep}{2}
\bibcite{dean2012large}{3}
\bibcite{kruger2013deep}{4}
\bibcite{stanford2016convnets}{5}
\bibcite{ng2015coursera}{6}
\bibcite{caffe2016loss}{7}
\bibstyle{abbrv}
